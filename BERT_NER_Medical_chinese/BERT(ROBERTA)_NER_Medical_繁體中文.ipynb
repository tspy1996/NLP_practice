{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0. 安裝套件\n",
    "[LINK](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. 簡易操作中文BERT\n",
    "[LINK](#p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. 如何將模型應用於下游任務: NER命名實體標示\n",
    "[LINK](#p2)\n",
    "\n",
    "#### 步驟：\n",
    "* 準備原始文本數據: 資料讀取與觀察\n",
    "* 資料前處理(簡體轉繁體、句子段落切分)\n",
    "* 將原始文本轉換成 BERT(RoBERTa) 相容的輸入格式、訓練模型、對新樣本做推論\n",
    "    * 實作 Dataset\n",
    "    * 實作 DataLoader\n",
    "    * Fine-tune Model\n",
    "    * 對新樣本做推論\n",
    "\n",
    "#### 描述：\n",
    "\n",
    "此程式將使用預先調整好的模型\n",
    "\n",
    "對中文的醫療語料庫進行命名實體識別(NER, Name Entity Recognition)\n",
    "\n",
    "模型是基於BERT模型，由於我們希望將模型遷移至**醫療病歷資料**使用，因此進行fine-tune訓練，使得模型能夠辨識醫療專有名詞\n",
    "\n",
    "資料集是由醫渡雲（北京）技術有限公司編寫，並由醫渡雲公司組織專業的醫學團隊進行人工標註，文字內容以癌症醫療影像檢查與結論為主\n",
    "\n",
    "整理後，訓練資料集包含800筆文字描述，並標記出文本中實體的起始位置與類型\n",
    "\n",
    "實體的類別包含:\n",
    "\n",
    "    1) 疾病和診斷：醫學上定義的疾病和醫生在臨床工作中對病因、病生理、分型分期等所作的判斷。\n",
    "\n",
    "    2) 影像檢查： 影像檢查（X線、CT、MR、PETCT等）+造影+超聲+心電圖，未避免檢查操作與手術操作過多衝突，不包含此外其它的診斷性操作，如胃鏡、腸鏡等。\n",
    "\n",
    "    3) 實驗室檢驗： 在實驗室進行的物理或化學檢查，本期特指臨床工作中檢驗科進行的化驗，不含免疫組化等廣義實驗室檢查\n",
    "\n",
    "    4) 手術： 醫生在患者身體局部進行的切除、縫合等治療，是外科的主要治療方法。\n",
    "\n",
    "    5) 藥物： 用於疾病治療的具體化學物質。\n",
    "\n",
    "    6) 解剖部位： 指疾病、症狀和體徵發生的人體解剖學部位。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. 安裝套件<a id='p0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我們先安裝必要的套件，包含 huggingface 的 transformers 以及 pytorch。\n",
    "\n",
    "huggingface transformers: https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.6.1\n",
    "# !pip install OpenCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mac 執行這行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "windows 執行這行:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch==1.8.1+cpu torchvision==0.9.1+cpu torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. 簡易操作中文BERT<a id='p1'></a>\n",
    "\n",
    "調用模型的方式可以直接引用套件後，指定模型名稱，下載官網的模型，\n",
    "使用哈工大訊飛聯合實驗室所預訓練的 bert-wwm 模型。\n",
    "\n",
    "HFL bert-wwm: https://github.com/ymcui/Chinese-BERT-wwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at hfl/chinese-bert-wwm and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForPreTraining(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
       "    )\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForPreTraining, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-bert-wwm')\n",
    "bert_model = AutoModelForPreTraining.from_pretrained(pretrained_model_name_or_path = \"hfl/chinese-bert-wwm\") \n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們將 input sentence 利用 tokenizer 變成 token，再把 token 轉成 index。這裡的 input sentence 可以換成任何你想要的句子。繁體或簡體都可以，因為預訓練時並沒有特別分繁簡體的資料集。\n",
    "\n",
    "關於更多的 tokenizer 使用方法可以參考官方文檔：https://huggingface.co/transformers/main_classes/tokenizer.html#pretrainedtokeni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['坐', '在', '原', '地', '等', '待', '機', '遇', '，', '無', '異', '於', '盼', '天', '上', '掉', '餡', '餅', '。', '毫', '不', '猶', '豫', '儘', '快', '拿', '出', '行', '動', '，', '為', '夢', '想', '的', '實', '現', '創', '造', '條', '件', '，', '才', '是', '夢', '想', '成', '真', '的', '必', '經', '之', '路', '。']\n",
      "[1777, 1762, 1333, 1765, 5023, 2521, 3582, 6878, 8024, 4192, 4530, 3176, 4687, 1921, 677, 2957, 7630, 7619, 511, 3690, 679, 4348, 6499, 1029, 2571, 2897, 1139, 6121, 1240, 8024, 4158, 1918, 2682, 4638, 2179, 4412, 1201, 6863, 3454, 816, 8024, 2798, 3221, 1918, 2682, 2768, 4696, 4638, 2553, 5195, 722, 6662, 511]\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"坐在原地等待機遇，無異於盼天上掉餡餅。毫不猶豫儘快拿出行動，為夢想的實現創造條件，才是夢想成真的必經之路。\"\n",
    "tokenized = bert_tokenizer.tokenize(input_sentence)\n",
    "ids = bert_tokenizer.convert_tokens_to_ids(tokenized)\n",
    "print(tokenized)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著我們把頭尾加上 special tokens\n",
    "\n",
    "([CLS]= 101, [SEP] = 102, [MASK] = 103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1777, 1762, 1333, 1765, 5023, 2521, 3582, 6878, 8024, 4192, 4530, 3176, 4687, 1921, 677, 2957, 7630, 7619, 511, 3690, 679, 4348, 6499, 1029, 2571, 2897, 1139, 6121, 1240, 8024, 4158, 1918, 2682, 4638, 2179, 4412, 1201, 6863, 3454, 816, 8024, 2798, 3221, 1918, 2682, 2768, 4696, 4638, 2553, 5195, 722, 6662, 511, 102]\n",
      "['[CLS]', '坐', '在', '原', '地', '等', '待', '機', '遇', '，', '無', '異', '於', '盼', '天', '上', '掉', '餡', '餅', '。', '毫', '不', '猶', '豫', '儘', '快', '拿', '出', '行', '動', '，', '為', '夢', '想', '的', '實', '現', '創', '造', '條', '件', '，', '才', '是', '夢', '想', '成', '真', '的', '必', '經', '之', '路', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "ids.insert(0, 101)\n",
    "ids.append(102)\n",
    "print(ids)\n",
    "print(bert_tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 接著我們定義一個方便我們做 masking 的函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(masked, start, end):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        masked: input sentence to be masked, should be a list of intergers\n",
    "        start: the starting position of mask\n",
    "        end: the ending position of mask\n",
    "    \"\"\"\n",
    "    if start < end:\n",
    "        for i in range(start,end + 1):\n",
    "            masked[i] = 103\n",
    "    else:\n",
    "        for i in range(end, start + 1):\n",
    "            masked[i] = 103  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "產生 mask 的句子。mask 函數的最後兩個 arguement 可以改成自己要的 start position 跟 end position。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['坐', '在', '原', '地', '等', '待', '機', '遇', '，', '無', '異', '於', '盼', '天', '上', '掉', '[MASK]', '[MASK]', '。', '毫', '不', '猶', '豫', '儘', '快', '拿', '出', '行', '動', '，', '為', '夢', '想', '的', '實', '現', '創', '造', '條', '件', '，', '才', '是', '夢', '想', '成', '真', '的', '必', '經', '之', '路', '。']\n"
     ]
    }
   ],
   "source": [
    "masked = ids.copy()\n",
    "mask(masked, 17, 18)\n",
    "tokens = bert_tokenizer.convert_ids_to_tokens(masked)[1:-1]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後把被 mask 過的 sequence 丟進 model 裡面，看看會預測出什麼東西。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['坐', '在', '原', '地', '等', '待', '機', '遇', '，', '無', '異', '於', '盼', '天', '上', '掉', '下', '石', '。', '毫', '不', '猶', '豫', '儘', '快', '拿', '出', '行', '動', '，', '為', '夢', '想', '的', '實', '現', '創', '造', '條', '件', '，', '才', '是', '夢', '想', '成', '真', '的', '必', '經', '之', '路', '。']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)\n",
    "x = torch.tensor([masked]).to(device)\n",
    "\n",
    "pred = bert_model(x)[0]\n",
    "pred = pred.argmax(-1).squeeze().cpu().numpy()\n",
    "pred_tokens = bert_tokenizer.convert_ids_to_tokens(pred)[1:-1]\n",
    "print(pred_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以把 input sentence 換成其他句子，把 [MASK] 放在其他地方，熟悉一下 BERT 的操作方式，並且觀察看看會發生什麼事。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. 如何將模型應用於下游任務: NER命名實體標示<a id=p2></a>\n",
    "在這個部分我們利用 chinese-roberta-wwm-ext作為基礎架構，進行中文電子病歷資料執行NER命名實體標示。\n",
    "\n",
    "調整過的模型已存放在資料夾 `./experiments/pretrain/`中，\n",
    "\n",
    "以下程式將著重在了解模型訓練需要的流程，並且直接載入調整過的模型，簡化訓練過程，\n",
    "\n",
    "### 直接使用模型進行標註可以直接跳至 [**對新樣本進行推論**](#INFERENCE)程式區塊\n",
    "\n",
    "訓練與測試原始資料為在資料夾`data`中的`train.json`，`test.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import config  # 事先在config.py檔案中，定義操作和訓練模型時的各項參數位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取原始資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[資料筆數] 800\n",
      "印出第一筆資料: \n",
      "\n",
      "[原始文字]\n",
      "\n",
      "，患者3月前因“直肠癌”于在我院于全麻上行直肠癌根治术（DIXON术），手术过程顺利，术后给予抗感染及营养支持治疗，患者恢复好，切口愈合良好。，术后病理示：直肠腺癌（中低度分化），浸润溃疡型，面积3.5*2CM，侵达外膜。双端切线另送“近端”、“远端”及环周底部切除面未查见癌。肠壁一站（10个）、中间组（8个）淋巴结未查见癌。，免疫组化染色示：ERCC1弥漫（+）、TS少部分弱（+）、SYN（-）、CGA（-）。术后查无化疗禁忌后给予3周期化疗，，方案为：奥沙利铂150MG D1，亚叶酸钙0.3G+替加氟1.0G D2-D6，同时给与升白细胞、护肝、止吐、免疫增强治疗，患者副反应轻。院外期间患者一般情况好，无恶心，无腹痛腹胀胀不适，无现患者为行复查及化疗再次来院就诊，门诊以“直肠癌术后”收入院。   近期患者精神可，饮食可，大便正常，小便正常，近期体重无明显变化。\n",
      "\n",
      "[標籤]\n",
      "\n",
      "[{'label_type': '疾病和诊断', 'overlap': 0, 'start_pos': 8, 'end_pos': 11}, {'label_type': '手术', 'overlap': 0, 'start_pos': 21, 'end_pos': 35}, {'label_type': '疾病和诊断', 'overlap': 0, 'start_pos': 78, 'end_pos': 95}, {'label_type': '解剖部位', 'overlap': 0, 'start_pos': 139, 'end_pos': 159}, {'end_pos': 234, 'label_type': '药物', 'overlap': 0, 'start_pos': 230}, {'end_pos': 247, 'label_type': '药物', 'overlap': 0, 'start_pos': 243}, {'end_pos': 255, 'label_type': '药物', 'overlap': 0, 'start_pos': 252}, {'label_type': '解剖部位', 'overlap': 0, 'start_pos': 276, 'end_pos': 277}, {'label_type': '解剖部位', 'overlap': 0, 'start_pos': 312, 'end_pos': 313}, {'label_type': '解剖部位', 'overlap': 0, 'start_pos': 314, 'end_pos': 315}, {'label_type': '疾病和诊断', 'overlap': 0, 'start_pos': 342, 'end_pos': 347}]\n"
     ]
    }
   ],
   "source": [
    "with open('./data/train.json',encoding='utf-8-sig') as read_file:\n",
    "    data = []\n",
    "    for line in read_file.readlines():\n",
    "        dic = json.loads(line)\n",
    "        data.append(dic)\n",
    "        \n",
    "print(f\"\"\"\n",
    "[資料筆數] {len(data)}\n",
    "印出第一筆資料: \n",
    "\n",
    "[原始文字]\n",
    "\n",
    "{data[0]['originalText']}\n",
    "\n",
    "[標籤]\n",
    "\n",
    "{data[0]['entities']}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 觀察資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'疾病和诊断': 3595,\n",
       " '手术': 813,\n",
       " '解剖部位': 7070,\n",
       " '药物': 1585,\n",
       " '影像检查': 865,\n",
       " '实验室检验': 1168}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看訓練資料各類別數量分布\n",
    "label_dict = {}\n",
    "for item in data:\n",
    "    for entity in item['entities']:\n",
    "        if entity['label_type'] not in label_dict:\n",
    "            label_dict[entity['label_type']] = 1\n",
    "        else:\n",
    "            label_dict[entity['label_type']] += 1\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以圖表顯示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#中文顯示設定\n",
    "# windows\n",
    "plt.rcParams['font.family'] = 'DFKai-SB' # 可體換字體 : DFKai-SB Microsoft JhengHei\n",
    "# Mac需改使用下面這行\n",
    "# plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "\n",
    "plt.rcParams.update({'font.size': 16}) # 字體大小\n",
    "plt.rcParams['axes.unicode_minus'] = False  # （解決座標軸負數的負號顯示問題）\n",
    "\n",
    "keys = label_dict.keys()\n",
    "values = label_dict.values()\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.bar(keys, values)\n",
    "plt.xticks(range(len(label_dict)), list(label_dict.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用OpenCC對簡體中文資料集，轉換成繁體中文資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "簡體轉繁體中文：\n",
      "[轉換前]\n",
      "，患者3月前因“直肠癌”于在我院于全麻上行直肠癌根治术（DIXON术），手术过程顺利，术后给予抗感染及营养支持治疗，患者恢复好，切口愈合良好。，术后病理示：直肠腺癌（中低度分化），浸润溃疡型，面积3.5*2CM，侵达外膜。双端切线另送“近端”、“远端”及环周底部切除面未查见癌。肠壁一站（10个）、中间组（8个）淋巴结未查见癌。，免疫组化染色示：ERCC1弥漫（+）、TS少部分弱（+）、SYN（-）、CGA（-）。术后查无化疗禁忌后给予3周期化疗，，方案为：奥沙利铂150MG D1，亚叶酸钙0.3G+替加氟1.0G D2-D6，同时给与升白细胞、护肝、止吐、免疫增强治疗，患者副反应轻。院外期间患者一般情况好，无恶心，无腹痛腹胀胀不适，无现患者为行复查及化疗再次来院就诊，门诊以“直肠癌术后”收入院。 近期患者精神可，饮食可，大便正常，小便正常，近期体重无明显变化。\n",
      "\n",
      "[轉換後]\n",
      "，患者3月前因“直腸癌”於在我院於全麻上行直腸癌根治術（DIXON術），手術過程順利，術後給予抗感染及營養支援治療，患者恢復好，切口癒合良好。，術後病理示：直腸腺癌（中低度分化），浸潤潰瘍型，面積3.5*2CM，侵達外膜。雙端切線另送“近端”、“遠端”及環周底部切除面未查見癌。腸壁一站（10個）、中間組（8個）淋巴結未查見癌。，免疫組化染色示：ERCC1瀰漫（+）、TS少部分弱（+）、SYN（-）、CGA（-）。術後查無化療禁忌後給予3週期化療，，方案為：奧沙利鉑150MG D1，亞葉酸鈣0.3G+替加氟1.0G D2-D6，同時給與升白細胞、護肝、止吐、免疫增強治療，患者副反應輕。院外期間患者一般情況好，無噁心，無腹痛腹脹脹不適，無現患者為行復查及化療再次來院就診，門診以“直腸癌術後”收入院。 近期患者精神可，飲食可，大便正常，小便正常，近期體重無明顯變化。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from opencc import OpenCC\n",
    "\n",
    "# 以一筆資料測試\n",
    "cc = OpenCC('s2twp') # s2twp: 簡體中文 -> 繁體中文 (台灣, 包含慣用詞轉換)\n",
    "\n",
    "sentence = '，患者3月前因“直肠癌”于在我院于全麻上行直肠癌根治术（DIXON术），手术过程顺利，术后给予抗感染及营养支持治疗，患者恢复好，切口愈合良好。，术后病理示：直肠腺癌（中低度分化），浸润溃疡型，面积3.5*2CM，侵达外膜。双端切线另送“近端”、“远端”及环周底部切除面未查见癌。肠壁一站（10个）、中间组（8个）淋巴结未查见癌。，免疫组化染色示：ERCC1弥漫（+）、TS少部分弱（+）、SYN（-）、CGA（-）。术后查无化疗禁忌后给予3周期化疗，，方案为：奥沙利铂150MG D1，亚叶酸钙0.3G+替加氟1.0G D2-D6，同时给与升白细胞、护肝、止吐、免疫增强治疗，患者副反应轻。院外期间患者一般情况好，无恶心，无腹痛腹胀胀不适，无现患者为行复查及化疗再次来院就诊，门诊以“直肠癌术后”收入院。 近期患者精神可，饮食可，大便正常，小便正常，近期体重无明显变化。'\n",
    "sentence_new = cc.convert(sentence)\n",
    "\n",
    "print(f\"\"\"\n",
    "簡體轉繁體中文：\n",
    "[轉換前]\n",
    "{sentence}\n",
    "\n",
    "[轉換後]\n",
    "{sentence_new}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 對整體資料及進行操作\n",
    "\n",
    "#### 先定義資料讀取路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path_train = './data/train.json'\n",
    "output_path_train = './data/train_s2twp.json' # 轉成繁體後輸出的檔名\n",
    "\n",
    "input_path_test = './data/test.json'\n",
    "output_path_test = './data/test_s2twp.json' # 轉成繁體後輸出的檔名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定義簡體轉繁體轉換函式 opencc_s2tw\n",
    "def opencc_s2tw(inputpath, outputpath):\n",
    "    \"\"\"\n",
    "    對訓練資料與測試資料執行簡體繁體轉換，除病歷資料外，也對標籤進行轉換\n",
    "    原始資料集中，\n",
    "    以 originalText 代表原始文字，\n",
    "    entities 表示標籤的名稱與對應原始文字的索引\n",
    "    \n",
    "    \"\"\"\n",
    "    # 讀資料\n",
    "    with open(inputpath, encoding='utf-8-sig') as read_file:\n",
    "        papers = []\n",
    "        for line in read_file.readlines():\n",
    "            dic = json.loads(line)\n",
    "            dic['originalText'] = cc.convert(dic['originalText'])\n",
    "\n",
    "            for entity in dic['entities']:\n",
    "                entity['label_type'] = cc.convert(entity['label_type'])\n",
    "            papers.append(dic)\n",
    "            \n",
    "    # 寫出資料到檔案 outputpath\n",
    "    with open(outputpath, 'w', encoding='utf-8-sig') as fp:\n",
    "        for line in papers:\n",
    "            line = json.dump(line,fp, ensure_ascii=False)\n",
    "            fp.write('\\n')\n",
    "            \n",
    "    print('----------------轉換成功，輸出檔案{}-------------------'.format(outputpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------轉換成功，輸出檔案./data/train_s2twp.json-------------------\n",
      "----------------轉換成功，輸出檔案./data/test_s2twp.json-------------------\n"
     ]
    }
   ],
   "source": [
    "# 呼叫函式opencc_s2tw\n",
    "opencc_s2tw(input_path_train, output_path_train)\n",
    "opencc_s2tw(input_path_test, output_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料集處理\n",
    "* 將資料處理成，NER任務需要的 BIO格式\n",
    "    * B-實體類別 : 表示實體的起點字\n",
    "    * I-實體類別 : 表示實體起點以外的字\n",
    "    * O : 非實體的字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取剛才轉完成的繁體檔案，輸出BIO格式資料，以npz檔案存放\n",
    "\n",
    "# 訓練集路徑\n",
    "input_path_train = './data/train_s2twp.json'\n",
    "output_path_train = './data/train_s2twp.npz'\n",
    "\n",
    "# 測試集路徑\n",
    "input_path_test = './data/test_s2twp.json'\n",
    "output_path_test = './data/test_s2twp.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定義一個將文字檔案轉成，BIO格式的函式\n",
    "以下程式會先將病歷文字，跟BIO格是label結合後，\n",
    "\n",
    "將資料每30個字切分成一筆資料，目的是減少訓練時間，\n",
    "\n",
    "並且捨棄掉切到標籤B、I中間位置的錯誤資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIOpreprocess(input_path, output_path):\n",
    "    MAX_LENGTH = config.max_length\n",
    "    word_list = []\n",
    "    label_list = []\n",
    "    with open(input_path, encoding='utf-8-sig') as f:\n",
    "        # 先讀取資料到記憶體中，再逐行處理\n",
    "        for line in f.readlines():\n",
    "            # loads()：用來處理記憶體中的json物件，strip去除可能存在的空格\n",
    "            json_line = json.loads(line.strip())\n",
    "            # 讀取 key= 'originalText'的資料\n",
    "            text = json_line['originalText']\n",
    "            words = list(text)\n",
    "            # 如果沒有label，則回傳none\n",
    "            label_entities = json_line.get('entities', None)\n",
    "            labels = ['O'] * len(words)\n",
    "\n",
    "            if label_entities is not None:\n",
    "                for i in label_entities:\n",
    "                    labels[i['start_pos']]='B-'+i['label_type']\n",
    "                    labels[i['start_pos']+1: i['end_pos']] = ['I-'+i['label_type']]*(i['end_pos']-i['start_pos']-1)\n",
    "\n",
    "            # 把文字跟label每 MAX_LENGTH 個字左右分成一組，縮小資料長度\n",
    "            sub_word = [words[i:i + MAX_LENGTH] for i in range(0, len(words), MAX_LENGTH)]\n",
    "            sub_label = [labels[i:i + MAX_LENGTH] for i in range(0, len(labels), MAX_LENGTH)]\n",
    "\n",
    "            # 刪除剛好切到label詞中間的資料\n",
    "            for w in range(len(sub_word)):\n",
    "                if sub_label[w][0] =='O' and sub_label[w][-1] =='O':\n",
    "                    word_list.append(sub_word[w])\n",
    "                    label_list.append(sub_label[w])\n",
    "                    \n",
    "\n",
    "        print(\"樣本數：\", len(word_list))\n",
    "        # 保存成二進位文件\n",
    "        np.savez_compressed(output_path, words=word_list, labels=label_list, dtype='object')\n",
    "        print(\"--------data process DONE! output_path: {}--------\".format(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 呼叫函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "樣本數： 8078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AICenterM365Lic\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------data process DONE! output_path: ./data/train_s2twp.npz--------\n",
      "樣本數： 1566\n",
      "--------data process DONE! output_path: ./data/test_s2twp.npz--------\n"
     ]
    }
   ],
   "source": [
    "BIOpreprocess(input_path_train, output_path_train)\n",
    "BIOpreprocess(input_path_test, output_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 由於示範僅能使用CPU運算\n",
    "#### 先選擇 10%資料作為訓練資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 資料切分，將訓練資料分離出 驗證集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with CPU\n",
      "--------data split DONE!--------\n",
      "訓練樣本數 727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def dev_split(dataset_dir):\n",
    "    \"\"\"split dev set\"\"\"\n",
    "    data = np.load(dataset_dir, allow_pickle=True)\n",
    "    words = data[\"words\"]\n",
    "    labels = data[\"labels\"]\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(words, labels, test_size=0.1, random_state=0)\n",
    "    return x_train, x_dev, y_train, y_dev\n",
    "\n",
    "def cpu_dev_split(dataset_dir):\n",
    "    \"\"\"split dev set\"\"\"\n",
    "    data = np.load(dataset_dir, allow_pickle=True)\n",
    "    words = data[\"words\"]\n",
    "    labels = data[\"labels\"]\n",
    "    \n",
    "    _, x_mini_train, _, y_mini_train = train_test_split(words, labels, test_size=0.1, random_state=0)\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x_mini_train, y_mini_train, test_size=0.1, random_state=0)\n",
    "    return x_train, x_dev, y_train, y_dev\n",
    "\n",
    "def load_dev(mode):\n",
    "    if mode == 'train with GPU':\n",
    "        word_train, word_dev, label_train, label_dev = dev_split(output_path_train)\n",
    "    elif mode =='train with CPU':\n",
    "        word_train, word_dev, label_train, label_dev = cpu_dev_split(output_path_train)\n",
    "    else:\n",
    "        word_train = None\n",
    "        label_train = None\n",
    "        word_dev = None\n",
    "        label_dev = None\n",
    "    print(\"--------data split DONE!--------\")\n",
    "    return word_train, word_dev, label_train, label_dev\n",
    "\n",
    "if config.device == torch.device('cpu') :\n",
    "    print('train with CPU')\n",
    "    word_train, word_dev, label_train, label_dev = load_dev('train with CPU')\n",
    "else:\n",
    "    word_train, word_dev, label_train, label_dev = load_dev('train with GPU')\n",
    "# 以參數 'train with CPU' 告訴函式選擇少量資料\n",
    "print('訓練樣本數',len(word_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將資料整理成 BERT模型能夠處理的形式\n",
    "#### 實作一個 `Dataset` 並將原始文本轉換成 BERT 相容的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先定義標籤對應的 id，以教學為目的先將標籤定義呈現在此，\n",
    "# 使用自己的資料集的話，需要同時變更這裡和 config.py內容\n",
    "\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-疾病和診斷\": 1,\n",
    "    \"B-影像檢查\": 2,\n",
    "    \"B-實驗室檢驗\": 3,\n",
    "    'B-手術': 4,\n",
    "    'B-藥物': 5,\n",
    "    'B-解剖部位': 6,\n",
    "    \"I-疾病和診斷\": 7,\n",
    "    \"I-影像檢查\": 8,\n",
    "    \"I-實驗室檢驗\": 9,\n",
    "    'I-手術': 10,\n",
    "    'I-藥物': 11,\n",
    "    'I-解剖部位': 12\n",
    "}\n",
    "\n",
    "id2label = {_id: _label for _label, _id in list(label2id.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 實做一個名為`NERDataset`的`Dataset` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "model_tokenizer = 'bert-base-chinese'\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, words, labels, word_pad_idx=0, label_pad_idx=-1):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained( model_tokenizer, do_lower_case=True) # 斷詞使用的模型\n",
    "        self.label2id = label2id\n",
    "        self.id2label = {_id: _label for _label, _id in list(label2id.items())}\n",
    "        self.dataset = self.preprocess(words, labels) # 呼叫 preprocess對原始資料進行轉換，得到可供模型訓練的格式\n",
    "        self.word_pad_idx = word_pad_idx\n",
    "        self.label_pad_idx = label_pad_idx\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def preprocess(self, origin_sentences, origin_labels):\n",
    "        \"\"\"\n",
    "        word: 將原始文字切成token，加入給模型識別用的特殊符號[CLS]，\n",
    "        sentence: 用tokenizer轉成文字編碼，並加入一個存放實際內文索引位置的array，讓模型知道需要處裡的文字位置\n",
    "        label: 將原始標籤，以我們是先定義的id對照表，取得數字形式的標籤\n",
    "        \n",
    "        examples: \n",
    "            word:['[CLS]', '浙', '商', '银', '行', '企', '业', '信', '贷', '部']\n",
    "            sentence:([101, 3851, 1555, 7213, 6121, 821, 689, 928, 6587, 6956],\n",
    "                        array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]))\n",
    "            label:[3, 13, 13, 13, 0, 0, 0, 0, 0]\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        \n",
    "        for line in origin_sentences:\n",
    "            words = []\n",
    "            word_lens = []\n",
    "            \n",
    "            # 對文字進行編碼\n",
    "            for token in line:\n",
    "                words.append(self.tokenizer.tokenize(token)) # 句子切分成單字\n",
    "                word_lens.append(len(token))\n",
    "                \n",
    "            # 變成單字列表，開頭加上特殊符號 [CLS]\n",
    "            words = ['[CLS]'] + [item for token in words for item in token]\n",
    "            \n",
    "            # 建立一個存放實際內文索引位置的array，讓模型識別需要處裡的文字位置\n",
    "            token_start_idxs = 1 + np.cumsum([0] + word_lens[:-1])\n",
    "            \n",
    "            # 組合編碼後的句子，與際內文索引位置的array\n",
    "            sentences.append((self.tokenizer.convert_tokens_to_ids(words), token_start_idxs))\n",
    "        \n",
    "        # 對文字標籤進行編碼\n",
    "        for tag in origin_labels:\n",
    "            label_id = [self.label2id.get(t) for t in tag]\n",
    "            labels.append(label_id)\n",
    "        \n",
    "        # 所有資料轉成 tensor 資料型態，模型運算需要\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            tmp = []\n",
    "            snt = []\n",
    "            sentence_tensor1 = torch.tensor(sentence[0])\n",
    "            sentence_tensor2 = torch.tensor(sentence[1])\n",
    "            tmp.append(sentence_tensor1)\n",
    "            tmp.append(sentence_tensor2)\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "            data.append((tmp, label))\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"sample data to get batch\"\"\"\n",
    "        words = self.dataset[idx][0]\n",
    "        \n",
    "        tokens_tensor = words[0]\n",
    "        segments_tensor = words[1]\n",
    "        label_tensor = self.dataset[idx][1]\n",
    "\n",
    "        return [tokens_tensor, segments_tensor, label_tensor]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"get dataset size\"\"\"\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將先前切分的訓練和驗證樣本，以`NERDataset`進行讀取和轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(word_train, label_train)\n",
    "dev_dataset = NERDataset(word_dev, label_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 看看第一個訓練樣本轉換前後的格式差異："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]\n",
      "['1', '（', '+', '+', '+', '）', '，', 'D', 'E', 'S', 'M', 'I', 'N', '（', '-', '）', '，', 'H', '-', 'C', 'A', 'L', 'D', 'E', 'S', 'M', 'O', 'N', '（', '-']\n",
      "--------------------\n",
      "\n",
      "[Dataset 回傳的 tensors]\n",
      "\n",
      "tokens_tensor \n",
      "長度: 31:\n",
      "\n",
      "tensor([ 101,  122, 8020,  116,  116,  116, 8021, 8024,  146,  147,  161,  155,\n",
      "         151,  156, 8020,  118, 8021, 8024,  150,  118,  145,  143,  154,  146,\n",
      "         147,  161,  155,  157,  156, 8020,  118])\n",
      "\n",
      "label_tensor \n",
      "長度: 30:\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "\n",
      "--------------------\n",
      "\n",
      "[還原 tokens_tensors]\n",
      "[CLS]1（+++），desmin（-），h-caldesmon（-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 選擇第一個樣本\n",
    "sample_idx = 0\n",
    "text = word_train[sample_idx] # 選擇第一筆尚未編碼的訓練資料\n",
    "\n",
    "tokens_tensor, mask, label_tensor = train_dataset[sample_idx] # 取得以 NERDataset進行讀取和轉換的結果\n",
    "\n",
    "# 使用 BertTokenizer 將 tokens_tensor 從編碼還原成文本\n",
    "tokenizer = BertTokenizer.from_pretrained(model_tokenizer, do_lower_case=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor)\n",
    "\n",
    "# 把單字組成句子\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "print(f\"\"\"[原始文本]\n",
    "{text}\n",
    "--------------------\n",
    "\n",
    "[Dataset 回傳的 tensors]\n",
    "\n",
    "tokens_tensor \n",
    "長度: {len(tokens_tensor)}:\n",
    "\n",
    "{tokens_tensor}\n",
    "\n",
    "label_tensor \n",
    "長度: {len(label_tensor)}:\n",
    "\n",
    "{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[還原 tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 實作一個 `DataLoader` \n",
    "\n",
    "實作可以一次回傳一個 mini-batch 的`DataLoader`，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data_loader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# 這個函式的輸入 `batch` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `NERDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor : 將文本加上特殊符號後，編碼後的結果\n",
    "# - segments_tensor : 原始文本在 tokens_tensor 中的索引\n",
    "# - label_tensor : 表示每個詞所對應的 label id\n",
    "def create_mini_batch(batch):\n",
    "    \"\"\"\n",
    "    將每個batch資料補成相同長度\n",
    "    \"\"\"\n",
    "    tokens_tensor = [x[0] for x in batch]\n",
    "    segments_tensor = [x[1] for x in batch]\n",
    "    label_tensor = [x[2] for x in batch]\n",
    "\n",
    "    # padding and aligning\n",
    "    # zero pad 到同一序列長度\n",
    "    batch_data = pad_sequence(tokens_tensor, batch_first=True)\n",
    "    \n",
    "    # 轉換segments_tensor，全部轉成 1，再 zero pad 到同一序列長度\n",
    "    # 表示有label位置=1，沒有的為 = 0\n",
    "    for i in segments_tensor:\n",
    "        i[i!=0] = 1 \n",
    "    batch_label_starts = pad_sequence(segments_tensor, batch_first=True)\n",
    "    \n",
    "    # label_tensor，以 -1 補到相同長度\n",
    "    batch_labels = pad_sequence(label_tensor, \n",
    "                                batch_first=True, padding_value=-1)\n",
    "\n",
    "    return batch_data, batch_label_starts, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Get Dataloader!--------\n"
     ]
    }
   ],
   "source": [
    "# build data_loader\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, collate_fn=create_mini_batch)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=create_mini_batch)\n",
    "\n",
    "print(\"--------Get Dataloader!--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 拿出一個 batch 看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[一個batch tensor尺寸]\n",
      "\n",
      "tokens_tensors : torch.Size([16, 31])\n",
      "segments_tensors.shape : torch.Size([16, 30])\n",
      "label_ids : torch.Size([16, 30])\n",
      "\n",
      "------------------------------------------------\n",
      "tokens_tensors: \n",
      "\n",
      "tensor([[ 101, 8024, 2642, 5442,  123,  121,  122,  123, 2399,  126, 3299, 1139,\n",
      "         4412, 1411, 1710, 1737, 7432, 8024,  845,  678, 5592, 6956,  679, 6900,\n",
      "         8024, 4158, 7403, 4578, 8024, 4192, 3209],\n",
      "        [ 101, 5440, 2719, 4158,  856, 1146, 1265, 7821, 4311, 5169, 5528, 4617,\n",
      "          511, 5631, 2414, 5440, 2719, 2152, 7534, 4617,  151,  151,  143,  123,\n",
      "         3309,  511,  123,  121,  122,  127,  118],\n",
      "        [ 101, 8024, 2642, 5442,  123,  121,  122,  126,  118,  121,  130,  118,\n",
      "          122,  126, 1728,  100, 5592, 4578,  845, 1920,  912, 2595, 4311, 5424,\n",
      "         2715, 3121, 6365,  122, 3299,  100, 2218],\n",
      "        [ 101,  510, 1164, 2228, 5023, 3780, 4615, 2527, 1962, 6752, 8024, 2456,\n",
      "         6359, 2642, 5442, 6868,  671, 3635, 2130, 1587, 1094, 5548, 6863, 2512,\n",
      "         8024,  852, 2642, 5442, 3313, 6905, 7015],\n",
      "        [ 101, 1461, 1429, 1737, 7432,  511,  791, 3189,  889, 7368, 2218, 6262,\n",
      "         8024, 4158, 6868,  671, 3635, 3780, 4615,  791, 3189, 1168, 2769, 4906,\n",
      "          857, 7368,  511, 2642, 5442, 5632, 4634],\n",
      "        [ 101, 1147, 4999, 1350, 1369, 6843, 8020,  678, 1147, 4999, 8021,  510,\n",
      "         8020,  677, 1147, 4999, 8021, 1772, 3313, 6210, 4617, 3863, 4056, 8024,\n",
      "         1369, 6843, 8020,  677, 1147, 4999, 8021],\n",
      "        [ 101,  809, 7028, 6213,  511,  128, 1921, 1184, 8024, 2642, 5442, 4192,\n",
      "         3209, 7549, 6294, 1728, 1086, 3613, 1139, 4412,  704,  678, 5592, 4563,\n",
      "         4578, 8024,  981, 3300, 2802, 1629, 8024],\n",
      "        [ 101, 8024, 2642, 5442,  857, 7368, 3309, 7279, 1265, 4615, 3681, 1199,\n",
      "         1353, 2746, 6738,  511,  791, 3189, 1057, 7368, 8024, 3093, 6121, 5018,\n",
      "          123, 3613, 1265, 4615,  511, 2642, 5442],\n",
      "        [ 101, 4192, 4634, 4229,  510, 3867, 4607,  510, 1286, 2527, 4671, 3731,\n",
      "         8024, 4192, 5541, 4578,  510, 7755, 4578,  510, 7531, 4578,  510, 5476,\n",
      "         1674, 8024, 4192, 4706, 4751,  677, 1795],\n",
      "        [ 101,  124,  119,  122,  125,  750, 5592, 5579, 5579, 1058, 3800, 2198,\n",
      "         3296, 1217, 3703,  122,  149,  115,  124, 1921,  116, 5584, 4606, 1889,\n",
      "         3647, 1728, 2094,  123,  121,  121, 5857],\n",
      "        [ 101, 4275, 4311, 2772,  745, 7531, 4311, 4495, 7269, 8024, 5169, 5528,\n",
      "         3300, 4530, 1798, 8024, 3417, 1146, 6162, 1008, 3211, 6210, 8024, 5178,\n",
      "         1394, 1048, 4554, 5175, 1265, 5178, 3362],\n",
      "        [ 101, 6210,  671, 5993,  679, 6211, 1179, 7384, 6629, 2595, 4567, 6365,\n",
      "         8024, 1920, 2207, 5147,  123,  119,  121,  115,  123,  119,  126,  145,\n",
      "          155,  511, 8024, 2527, 3176,  123,  121],\n",
      "        [ 101, 7755, 3178, 5541, 1880, 5584, 4289, 8024, 5440, 2719, 6752, 4919,\n",
      "         8024, 6733, 1184, 4685,  820, 8039, 7534, 6956, 1914, 4634, 2207, 3900,\n",
      "         2349, 5178, 8024, 6733, 1184, 4924, 1872],\n",
      "        [ 101,  118,  162,  163,  144,  163,  154,  151,  156,  118,  151,  151,\n",
      "          151, 8020,  116, 8021,  510,  162,  161, 8020,  118, 8021,  510,  160,\n",
      "          160,  155,  122, 8020,  118, 8021,  510],\n",
      "        [ 101, 1453, 1752, 3313, 1350, 3209, 7549, 5584, 1920, 3900, 2349, 5178,\n",
      "         8024, 6123, 2527,  750, 7521, 7344, 2595, 2834, 2697, 3381,  510, 1169,\n",
      "         7000,  510, 1048, 4554, 6310, 5059, 1350],\n",
      "        [ 101, 1366, 3302,  146,  122,  118,  146,  122,  125, 8024, 1265, 4615,\n",
      "         7279, 3309, 1139, 4412,  151,  151,  180, 1679, 2552, 1653, 1402, 8024,\n",
      "          886, 4500, 6249, 4180, 1385, 4475, 5474]])\n",
      "------------------------------------------------\n",
      "segments_tensors:\n",
      "\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], dtype=torch.int32)\n",
      "------------------------------------------------\n",
      "label_ids:\n",
      "\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          6, 12, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  1,  7,  7,  7,  7,  7,  7,  7,  0,  0,  0,  0,  0,  1,  7,\n",
      "          7,  7,  7,  7,  7,  7,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  2,  8,  8,  8,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  6, 12, 12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  0,  6,  0,\n",
      "          0,  6,  0,  0,  0,  0,  0,  0,  6, 12,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  6, 12,  0,  0,  0,  0,  5, 11, 11,  0,  0,  0,  0,\n",
      "          0,  0,  5, 11, 11, 11, 11, 11,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6, 12,\n",
      "          0,  0,  0,  6, 12, 12,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  5, 11, 11, 11,  0]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "\n",
    "tokens_tensors, segments_tensors, label_ids = data\n",
    "# - tokens_tensor : 將文本加上特殊符號後，編碼後的結果，0代表空的區域\n",
    "# - segments_tensor : 一個批次的資料經過長度補齊後，以1表示文本位置，0代表空的區域\n",
    "# - label_tensor : 編碼後的標籤，-1代表空的區域\n",
    "\n",
    "# tokens_tensors包含[CLS]特殊符號，所以長度多 1\n",
    "print(f\"\"\"\n",
    "[一個batch tensor尺寸]\n",
    "\n",
    "tokens_tensors : {tokens_tensors.shape}\n",
    "segments_tensors.shape : {segments_tensors.shape}\n",
    "label_ids : {label_ids.shape}\n",
    "\n",
    "------------------------------------------------\n",
    "tokens_tensors: \n",
    "\n",
    "{tokens_tensors}\n",
    "------------------------------------------------\n",
    "segments_tensors:\n",
    "\n",
    "{segments_tensors}\n",
    "------------------------------------------------\n",
    "label_ids:\n",
    "\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立模型\n",
    "在BERT模型架構下，加入分類器和CRF運算層，達成NER任務\n",
    "\n",
    "CRF(Conditional Random Field) 條件隨機場是自然語言處理領域常用的算法之一，常用於句法分析、命名實體識別、詞性標註等。\n",
    "\n",
    "[CRF原理補充](https://zhuanlan.zhihu.com/p/44042528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義模型架構\n",
    "\n",
    "# from transformers.modeling_bert import *\n",
    "from transformers.models.bert.modeling_bert import *  # 依套件版本不同，import方式可能為此行或前一行\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchcrf import CRF\n",
    "\n",
    "class BertNER(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertNER, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_data, token_type_ids=None, attention_mask=None, labels=None,\n",
    "                position_ids=None, inputs_embeds=None, head_mask=None):\n",
    "        \n",
    "        input_ids, input_token_starts = input_data\n",
    "\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask,\n",
    "                            inputs_embeds=inputs_embeds)\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        # 去除[CLS]標籤等位置，獲得和label對齊的pre_label表示\n",
    "        origin_sequence_output = [layer[starts.nonzero().squeeze(1)]\n",
    "                                  for layer, starts in zip(sequence_output, input_token_starts)]\n",
    "        # 將sequence_output的pred_label维度padding到最大長度\n",
    "        padded_sequence_output = pad_sequence(origin_sequence_output, batch_first=True)\n",
    "        # dropout pred_label的一部分feature\n",
    "        padded_sequence_output = self.dropout(padded_sequence_output)\n",
    "        # 得到判別值\n",
    "        logits = self.classifier(padded_sequence_output)\n",
    "        \n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            loss_mask = labels.gt(-1)\n",
    "            loss = self.crf(logits, labels, loss_mask) * (-1)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        # contain: (loss), scores\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練模型\n",
    "上面定義了模型的架構後，以Huggingface的RoBERTa模型(對BERT改良後的模型之一)參數為基礎，對我們的資料及進行學習。\n",
    "\n",
    "事先將RoBERTa模型fine-tune後，存放在'experiments/pretrain/' 中，下面程式會直接調用進行訓練\n",
    "\n",
    "[RoBERTa模型來源 chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 訓練模型前，初始化建立模型，決定訓練時的超參數和optimizer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers.optimization import get_cosine_schedule_with_warmup, AdamW\n",
    "\n",
    "# 得到訓練資料長度\n",
    "train_size = len(train_dataset)\n",
    "\n",
    "# Prepare model\n",
    "# 定義初始模型權重，指定模型存放的路徑\n",
    "MODEL_PATH = 'experiments/pretrain/'\n",
    "\n",
    "# 將預訓練模型和需要分類的類別數量，提供給方才定義的BertNER模型架構\n",
    "model = BertNER.from_pretrained( MODEL_PATH, num_labels=len(config.label2id))\n",
    "\n",
    "# 使用 GPU或是CPU運算\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "model.to(device)\n",
    "\n",
    "# Prepare optimizer\n",
    "# 訓練模型時的超參數設定，可在config.py檔案中調整\n",
    "if config.full_fine_tuning:\n",
    "    # model.named_parameters(): [bert, classifier, crf]\n",
    "    bert_optimizer = list(model.bert.named_parameters())\n",
    "    classifier_optimizer = list(model.classifier.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in bert_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': config.weight_decay},\n",
    "        {'params': [p for n, p in bert_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in classifier_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'lr': config.learning_rate * 5, 'weight_decay': config.weight_decay},\n",
    "        {'params': [p for n, p in classifier_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'lr': config.learning_rate * 5, 'weight_decay': 0.0},\n",
    "        {'params': model.crf.parameters(), 'lr': config.learning_rate * 5}\n",
    "    ]\n",
    "# only fine-tune the head classifier\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate, correct_bias=False)\n",
    "train_steps_per_epoch = train_size // BATCH_SIZE\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=(config.epoch_num // 10) * train_steps_per_epoch,\n",
    "                                            num_training_steps=config.epoch_num * train_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以下定義三個函式`train()`、`train_epoch()`，`evaluate()`用來處理模型訓練時，\n",
    "#### 計算每個epoch結果在驗證集上的表現、調用optimizer更新模型中的參數等訓練流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from metrics import f1_score \n",
    "\n",
    "def train_epoch(train_loader, model, optimizer, scheduler, epoch):\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    train_losses = 0\n",
    "    \n",
    "    for idx, batch_samples in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        batch_data, batch_token_starts, batch_labels = batch_samples\n",
    "        \n",
    "        # shift tensors to GPU if available\n",
    "        batch_data, batch_token_starts = batch_data.to(config.device), batch_token_starts.to(config.device)\n",
    "        batch_labels = batch_labels.to(config.device)\n",
    "        \n",
    "        batch_masks = batch_data.gt(0)  # get padding mask\n",
    "        \n",
    "        # compute model output and loss # forward pass\n",
    "        loss = model((batch_data, batch_token_starts),\n",
    "                     token_type_ids=None, attention_mask=batch_masks, labels=batch_labels)[0]\n",
    "        \n",
    "        train_losses += loss.item()\n",
    "        \n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=config.clip_grad)\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    train_loss = float(train_losses) / len(train_loader)\n",
    "    \n",
    "    print(\"Epoch: {}, train loss: {}\".format(epoch, train_loss))\n",
    "\n",
    "\n",
    "def train(train_loader, dev_loader, model, optimizer, scheduler, model_dir):\n",
    "    \"\"\"train the model and test model performance\"\"\"\n",
    "    # reload weights from restore_dir if specified\n",
    "    if model_dir is not None and config.load_before:\n",
    "        model = BertNER.from_pretrained(model_dir)\n",
    "        model.to(config.device)\n",
    "        print(\"--------Load model from {}--------\".format(model_dir))\n",
    "        \n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    # start training\n",
    "    for epoch in range(1, config.epoch_num+ 1): # config.epoch_num\n",
    "        train_epoch(train_loader, model, optimizer, scheduler, epoch)\n",
    "        \n",
    "        val_metrics = evaluate(dev_loader, model, mode='dev')\n",
    "        val_f1 = val_metrics['f1']\n",
    "        print(\"Epoch: {}, dev loss: {}, f1 score: {}\".format(epoch, val_metrics['loss'], val_f1))\n",
    "        improve_f1 = val_f1 - best_val_f1\n",
    "        if improve_f1 > 1e-5:\n",
    "            best_val_f1 = val_f1\n",
    "            model.save_pretrained(model_dir)\n",
    "            \n",
    "            print(\"--------Save best model!--------\")\n",
    "            if improve_f1 < config.patience:\n",
    "                patience_counter += 1\n",
    "            else:\n",
    "                patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        # Early stopping and logging best f1\n",
    "        if (patience_counter >= config.patience_num and epoch > config.min_epoch_num) or epoch == config.epoch_num:\n",
    "            print(\"Best val f1: {}\".format(best_val_f1))\n",
    "            break\n",
    "    print(\"Training Finished!\")\n",
    "\n",
    "\n",
    "def evaluate(dev_loader, model, mode='dev'):\n",
    "    \n",
    "    model.eval()\n",
    "    if mode == 'test':\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_tokenizer, do_lower_case=True, skip_special_tokens=True)\n",
    "    \n",
    "    id2label = config.id2label\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    sent_data = []\n",
    "    dev_losses = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch_samples in enumerate(tqdm(dev_loader)):\n",
    "            batch_data, batch_token_starts, batch_tags = batch_samples\n",
    "            # shift tensors to GPU if available\n",
    "            batch_data, batch_token_starts = batch_data.to(config.device), batch_token_starts.to(config.device)\n",
    "            batch_tags = batch_tags.to(config.device)\n",
    "            \n",
    "            if mode == 'test':\n",
    "                sent_data.extend([[tokenizer.convert_ids_to_tokens(idx.item()) for idx in indices\n",
    "                                   if (idx.item() > 0 and idx.item() != 101)] for indices in batch_data])\n",
    "                \n",
    "            batch_masks = batch_data.gt(0)  # get padding mask, gt(x): get index greater than x\n",
    "            label_masks = batch_tags.gt(-1)  # get padding mask, gt(x): get index greater than x\n",
    "            \n",
    "            # compute model output and loss\n",
    "            loss = model((batch_data, batch_token_starts),\n",
    "                         token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)[0]\n",
    "            dev_losses += loss.item()\n",
    "            # (batch_size, max_len, num_labels)\n",
    "            batch_output = model((batch_data, batch_token_starts),\n",
    "                                 token_type_ids=None, attention_mask=batch_masks)[0]\n",
    "            # (batch_size, max_len - padding_label_len)\n",
    "            batch_output = model.crf.decode(batch_output, mask=label_masks)\n",
    "            # (batch_size, max_len)\n",
    "            batch_tags = batch_tags.to('cpu').numpy()\n",
    "            pred_tags.extend([[id2label.get(idx) for idx in indices] for indices in batch_output])\n",
    "            # (batch_size, max_len - padding_label_len)\n",
    "            true_tags.extend([[id2label.get(idx) for idx in indices if idx > -1] for indices in batch_tags])\n",
    "\n",
    "    assert len(pred_tags) == len(true_tags)\n",
    "    if mode == 'test':\n",
    "        assert len(sent_data) == len(true_tags)\n",
    "\n",
    "    # logging loss, f1 and report\n",
    "    metrics = {}\n",
    "    if mode == 'dev':\n",
    "        f1 = f1_score(true_tags, pred_tags, mode)\n",
    "        metrics['f1'] = f1\n",
    "    else:\n",
    "        f1_labels, f1 = f1_score(true_tags, pred_tags, mode)\n",
    "        metrics['f1_labels'] = f1_labels\n",
    "        metrics['f1'] = f1\n",
    "    metrics['loss'] = float(dev_losses) / len(dev_loader)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 呼叫函式`train()`訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Start Training!--------\n",
      "batch_size 16\n",
      "epoch_num 3\n",
      "min_epoch_num 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [02:30<00:00,  3.27s/it]\n",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 108.99300670105478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:10<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, dev loss: 65.24098652601242, f1 score: 0.7236842105263159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Save best model!--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [02:54<00:00,  3.79s/it]\n",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train loss: 108.30063150240028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.06s/it]\n",
      "  0%|                                                                                           | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, dev loss: 63.37012751897176, f1 score: 0.7189542483660132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [03:34<00:00,  4.67s/it]\n",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, train loss: 99.35403533603834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:13<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, dev loss: 61.14374351501465, f1 score: 0.7320261437908496\n",
      "--------Save best model!--------\n",
      "Best val f1: 0.7320261437908496\n",
      "Training Finished!\n"
     ]
    }
   ],
   "source": [
    "# # Train the model\n",
    "# print(\"--------Start Training!--------\")\n",
    "# # 更改預設 epoch\n",
    "# config.epoch_num = 2\n",
    "# config.min_epoch_num = 2\n",
    "# print('batch_size',BATCH_SIZE)\n",
    "# print('epoch_num',config.epoch_num)\n",
    "# print('min_epoch_num', config.min_epoch_num)\n",
    "\n",
    "# train(train_loader, dev_loader, model, optimizer, scheduler, config.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用測試集資料查看模型表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dir, mode):\n",
    "    data = np.load(test_dir, allow_pickle=True)\n",
    "    word_test = data[\"words\"]\n",
    "    label_test = data[\"labels\"]\n",
    "    \n",
    "    if mode != 'GPU':\n",
    "        _, x_mini_test, _, y_mini_test = train_test_split(word_test, label_test, test_size=0.3, random_state=10)\n",
    "        word_test = x_mini_test\n",
    "        label_test = y_mini_test  \n",
    "    test_dataset = NERDataset(word_test, label_test, config)\n",
    "    print(\"--------Dataset Build!--------\")\n",
    "    # build data_loader\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, collate_fn=create_mini_batch)\n",
    "    print(\"--------Get Data-loader!--------\")\n",
    "    # Prepare model\n",
    "    if config.model_dir is not None:\n",
    "        model = BertNER.from_pretrained(config.model_dir)\n",
    "        model.to(config.device)\n",
    "        print(\"--------Load model from {}--------\".format(config.model_dir))\n",
    "    else:\n",
    "        print(\"--------No model to test !--------\")\n",
    "        return\n",
    "    \n",
    "    val_metrics = evaluate(test_loader, model, mode='test')\n",
    "    val_f1 = val_metrics['f1']\n",
    "    print(\"test loss: {}, f1 score: {}\".format(val_metrics['loss'], val_f1))\n",
    "\n",
    "    val_f1_labels = val_metrics['f1_labels']\n",
    "    for label in config.labels:\n",
    "        print(\"f1 score of {}: {}\".format(label, val_f1_labels[label]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Dataset Build!--------\n",
      "--------Get Data-loader!--------\n",
      "--------Load model from experiments/pretrain/--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:52<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 123.60875466664632, f1 score: 0.7577777777777779\n",
      "f1 score of 疾病和診斷: 0.7078651685393258\n",
      "f1 score of 影像檢查: 0.6451612903225806\n",
      "f1 score of 實驗室檢驗: 0.42105263157894735\n",
      "f1 score of 手術: 0.5964912280701754\n",
      "f1 score of 藥物: 0.7555555555555555\n",
      "f1 score of 解剖部位: 0.8114285714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = './data/test_s2twp.npz' #測試資料路徑\n",
    "\n",
    "#依序硬體選擇使用的資料量\n",
    "if config.device == torch.device('cpu'):\n",
    "    test(test_dir, 'CPU')\n",
    "else:\n",
    "    test(test_dir, 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 對新樣本進行推論 (INFERENCE)<a id = INFERENCE></a>\n",
    "* 使用訓練好的模型，自行輸入醫療文字\n",
    "* 查看模型預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import get_entities\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from model import BertNER\n",
    "import config\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', do_lower_case=True)\n",
    "        self.label2id = config.label2id\n",
    "        self.id2label = {_id: _label for _label, _id in list(config.label2id.items())}\n",
    "        \n",
    "        self.model = BertNER.from_pretrained(config.model_dir) #載入訓練好的模型\n",
    "        self.model.to(config.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def predict(self, text):\n",
    "\n",
    "        raw_words = self.tokenizer.tokenize(text)\n",
    "\n",
    "        word_len = len(raw_words)\n",
    "        words = ['[CLS]'] + raw_words\n",
    "        words = [self.tokenizer.convert_tokens_to_ids(words)]\n",
    "        words = torch.tensor(words, dtype=torch.long,device=self.device)\n",
    "        token_start_idxs = [np.arange(start=1, stop=word_len+1)]\n",
    "        token_start_idxs =torch.tensor(token_start_idxs,dtype=torch.long,device=self.device)\n",
    "        data = (words,token_start_idxs)\n",
    "        \n",
    "        pred_tags = []\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            output = self.model(data)[0]\n",
    "            # (batch_size, max_len - padding_label_len)\n",
    "            label_masks = token_start_idxs.gt(-1)\n",
    "        \n",
    "        output = self.model.crf.decode(output, mask=label_masks)\n",
    "        pred_tags.extend([[self.id2label.get(idx) for idx in indices] for indices in output])\n",
    "        \n",
    "        pred_entities = set(get_entities(pred_tags))\n",
    "\n",
    "        print(f\"\"\"\n",
    "[原始文字]\n",
    "{text}\"\"\")\n",
    "        \n",
    "        for i in pred_entities:\n",
    "            start_id = i[1]\n",
    "            end = i[2]\n",
    "            a = ''.join(raw_words[start_id: end+1])\n",
    "            print(f\"\"\"\n",
    "[預測實體內容] {a} \n",
    "[標籤] {i[0]} [起始] {start_id} [結束] {end}\"\"\")\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[原始文字]\n",
      "入院後給予抑酸、促進粘膜修復等藥物治療，行胃鏡檢查提示慢性萎縮性胃炎，食管乳頭狀瘤，未予處理，病情好轉出院。出院後未再複查，未系統服藥，時有下腹部不適，現為行“食管乳頭狀瘤”胃鏡上切除來院，門診以“食管息肉”收入院。\n",
      "\n",
      "[預測實體內容] 食管息肉 \n",
      "[標籤] 疾病和診斷 [起始] 99 [結束] 102\n",
      "\n",
      "[預測實體內容] 下腹部 \n",
      "[標籤] 解剖部位 [起始] 70 [結束] 72\n",
      "\n",
      "[預測實體內容] 慢性萎縮性胃炎 \n",
      "[標籤] 疾病和診斷 [起始] 27 [結束] 33\n",
      "\n",
      "[預測實體內容] 食管乳頭狀瘤 \n",
      "[標籤] 疾病和診斷 [起始] 35 [結束] 40\n",
      "\n",
      "[預測實體內容] 食管乳頭狀瘤 \n",
      "[標籤] 疾病和診斷 [起始] 80 [結束] 85\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "# 提供以下三段文字可以替換，觀察模型表現\n",
    "text1 = '患者3月前因“直腸癌”於在我院於全麻上行直腸癌根治術（DIXON術），手術過程順利，術後給予抗感染及營養支援治療，患者恢復好，切口癒合良好。'\n",
    "text2 = '入院後給予抑酸、促進粘膜修復等藥物治療，行胃鏡檢查提示慢性萎縮性胃炎，食管乳頭狀瘤，未予處理，病情好轉出院。出院後未再複查，未系統服藥，時有下腹部不適，現為行“食管乳頭狀瘤”胃鏡上切除來院，門診以“食管息肉”收入院。'\n",
    "text3 = '入院前1月餘因“肝內外膽管結石伴急性膽管炎”在全麻上行“膽總管切開取石+術中膽道鏡取石+左肝部分切除+右肝內葉‘十字’切開取石”術，術順'\n",
    "\n",
    "result = model.predict(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
