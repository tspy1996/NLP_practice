{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02733277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. content done\n",
      "2. push done\n"
     ]
    }
   ],
   "source": [
    "# 讀資料庫 >> df\n",
    "# 讀exp 資料\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "db_settings = {\n",
    "        \"host\": \"\",\n",
    "        \"port\": ,\n",
    "        \"user\": \"\",\n",
    "        \"password\": \"\",\n",
    "        \"db\": \"\",\n",
    "        \"charset\": \"utf8\"\n",
    "        }\n",
    "\n",
    "db_timeformat = \"%Y-%m-%d %H:%M:%S\"\n",
    "conn = pymysql.connect(**db_settings)\n",
    "\n",
    "with conn.cursor() as cursor:\n",
    "    sql = \"SELECT * FROM `ptt_content` WHERE `tp` BETWEEN '2021-07-01' AND '2021-07-10'\"\n",
    "    cursor.execute(sql)\n",
    "    r = cursor.fetchall()\n",
    "    content_df = pd.DataFrame(list(r),\n",
    "                              columns=['url', 'title', 'content', 'time', 'author', 'board', 'ip', 'country', 'push_count'])\n",
    "\n",
    "    print('1. content done')\n",
    "    sql = \"SELECT * FROM `ptt_push` WHERE `tp` BETWEEN '2021-07-01' AND '2021-07-10'\"\n",
    "    cursor.execute(sql)\n",
    "    r = cursor.fetchall()\n",
    "    push_df = pd.DataFrame(list(r),\n",
    "                           columns=['url', 'seq', 'board', 'tag', 'content', 'time', 'userid', 'reply'])\n",
    "    print('2. push done')\n",
    "    \n",
    "with conn.cursor() as cursor:\n",
    "    sql = \"SELECT * FROM `news` WHERE `published_time` BETWEEN '2021-07-01' AND '2021-07-10'\"\n",
    "    cursor.execute(sql)\n",
    "    r = cursor.fetchall()\n",
    "    news_df = pd.DataFrame(list(r),\n",
    "                              columns=['url', 'title', 'content', 'author', 'board', 'tag','source','published_time','collected_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b632164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. train_content_df done\n"
     ]
    }
   ],
   "source": [
    "# 讀ptt訓練資料\n",
    "db_settings = {\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 3306,\n",
    "        \"user\": \"root\",\n",
    "        \"password\": \"ai52004800\",\n",
    "        \"db\": \"lab\",\n",
    "        \"charset\": \"utf8\"\n",
    "        }\n",
    "\n",
    "db_timeformat = \"%Y-%m-%d %H:%M:%S\"\n",
    "conn = pymysql.connect(**db_settings)\n",
    "\n",
    "with conn.cursor() as cursor:\n",
    "    sql = \"SELECT * FROM `ptt_content` WHERE `tp` BETWEEN '2021-01-01' AND '2021-07-01'\"\n",
    "    cursor.execute(sql)\n",
    "    r = cursor.fetchall()\n",
    "    train_content_df = pd.DataFrame(list(r),\n",
    "                              columns=['url', 'title', 'content', 'time', 'author', 'board', 'ip', 'country', 'push_count'])\n",
    "    print('1. train_content_df done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7bfb71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import pickle\n",
    "import jieba\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5784f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\AICENT~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.771 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "for f in listdir(\"./predefined/dictionary/\"):\n",
    "    jieba.load_userdict(\"./predefined/dictionary/\"+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f9c35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_wd = set()\n",
    "with open(\"./predefined/stopwords_all.json\", 'r', encoding='utf8') as f:\n",
    "    for w in json.load(f):\n",
    "        stop_wd.add(w)\n",
    "        \n",
    "with open(\"./predefined/stopwords_news.json\", 'r', encoding='utf8') as f:\n",
    "    for w in json.load(f):\n",
    "        stop_wd.add(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce03f3",
   "metadata": {},
   "source": [
    "#### filter stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "629f4e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864b13a7c6cf466fa70f1701511a4259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/239094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "article_data = pd.concat([train_content_df.content, news_df.content]).reset_index(drop=True)\n",
    "# 斷詞轉換寫檔\n",
    "with open('article_data_text.txt', 'w', encoding='utf-8') as f:\n",
    "    for article in tqdm(article_data):\n",
    "\n",
    "        article = article.strip()\n",
    "        data = jieba.cut(article)\n",
    "        data = [word for word in data if word != ' ' and  word not in stop_wd]\n",
    "        data = ' '.join(data)\n",
    "            \n",
    "        f.write(data+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb0be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.corpora import WikiCorpus\n",
    "# # https://clay-atlas.com/blog/2020/01/17/python-chinese-tutorial-gensim-word2vec/\n",
    "# # wiki輔助訓練word2vec 模型\n",
    "# wiki_corpus = WikiCorpus(r'C:\\Users\\AICenterM365Lic\\Downloads\\zhwiki-20210701-pages-articles-multistream.xml.bz2', dictionary={})\n",
    "# text_num = 0\n",
    "\n",
    "# with open('wiki_text.txt', 'w', encoding='utf-8') as f:\n",
    "#     for text in wiki_corpus.get_texts():\n",
    "#         f.write(' '.join(text)+'\\n')\n",
    "#         text_num += 1\n",
    "#         if text_num % 10000 == 0:\n",
    "#             print('{} articles processed.'.format(text_num))\n",
    "\n",
    "#     print('{} articles processed.'.format(text_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8357029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184bcc7114b94cbda6ade5cbedb39b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from opencc import OpenCC\n",
    "# Initial\n",
    "cc = OpenCC('s2t')\n",
    "# Tokenize\n",
    "with open('article_data_text.txt', 'a', encoding='utf-8') as new_f:\n",
    "    with open('wiki_text.txt', 'r', encoding='utf-8') as f:\n",
    "        for data in tqdm(f):\n",
    "            data = cc.convert(data)\n",
    "            data = jieba.cut(data)\n",
    "            data = [word for word in data if word != ' ' and word not in stop_wd]\n",
    "            data = ' '.join(data)\n",
    "\n",
    "            new_f.write(data+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103c473",
   "metadata": {},
   "source": [
    "### w2v訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49f629e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save corpus_data\n",
    "# with open(\"./corpus.txt\", 'w') as f:\n",
    "#     for i in new_corpus:\n",
    "#         f.write(str(i)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cff860a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # 讀進wiki資料\n",
    "# with open('./predefined/dictionary/ptt.txt', 'r', encoding='utf-8') as new_f:\n",
    "#     wiki_data = [f for f in new_f.read()]\n",
    "    \n",
    "# wiki_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b7dee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練中...\n",
      "model 已儲存完畢\n"
     ]
    }
   ],
   "source": [
    "# 主要透過 gensim 訓練成 model 並供使用\n",
    "class Train(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # 可參考 https://radimrehurek.com/gensim/models/word2vec.html 更多運用\n",
    "    def train(self):\n",
    "        print(\"訓練中...\")\n",
    "        # Load file\n",
    "        sentence = word2vec.LineSentence(\"article_data_text.txt\")\n",
    "        # Setting degree and Produce Model(Train)\n",
    "        # Settings\n",
    "        seed = 666\n",
    "        sg = 0\n",
    "        window_size = 10\n",
    "        vector_size = 100\n",
    "        min_count = 1\n",
    "        workers = 8\n",
    "        epochs = 5\n",
    "        batch_words = 10000\n",
    "        \n",
    "        model = word2vec.Word2Vec(\n",
    "                sentence,\n",
    "                min_count=min_count,\n",
    "                vector_size=vector_size,\n",
    "                workers=workers,\n",
    "                epochs=epochs,\n",
    "                window=window_size,\n",
    "                sg=sg,\n",
    "                seed=seed,\n",
    "                batch_words=batch_words\n",
    "                )\n",
    "        # Save model \n",
    "        model.save('word2vec.model')\n",
    "        model.wv.save_word2vec_format(u\"w2v.model.bin\", binary = True)\n",
    "        print(\"model 已儲存完畢\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = Train()\n",
    "    # 訓練(shallow semantic space)\n",
    "    t.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b3b949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "# How to use bin(model)?\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"w2v.model.bin\", binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18882fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "外文 前10名相似:\n",
      "西文,0.7729334831237793\n",
      "中英文,0.6763495206832886\n",
      "中文書籍,0.6715853214263916\n",
      "o9NO7D,0.664551317691803\n",
      "譯自,0.6533045172691345\n",
      "中文,0.652868926525116\n",
      "中文期刊,0.6455404162406921\n",
      "英語翻譯,0.6444995403289795\n",
      "譯寫,0.638895571231842\n",
      "翻譯,0.6315523982048035\n",
      "筆譯,0.6294631958007812\n",
      "刊名,0.6286423802375793\n",
      "英文翻譯,0.6213029623031616\n",
      "簡體,0.6150707006454468\n",
      "專業書籍,0.6088368892669678\n",
      "譯版,0.6057499051094055\n",
      "察布查爾報,0.6038843989372253\n",
      "中文名,0.6009138822555542\n",
      "英漢對照,0.6000564098358154\n",
      "對譯,0.5964251160621643\n"
     ]
    }
   ],
   "source": [
    "print(\"外文 前10名相似:\")    \n",
    "res = word_vectors.most_similar('外文', topn = 20)\n",
    "for item in res:\n",
    "    print(item[0] + \",\" + str(item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df92d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f35e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646df1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533ef8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
